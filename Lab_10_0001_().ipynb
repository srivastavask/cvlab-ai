{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mVcgqYbNcro",
        "outputId": "1b9111f3-7947-4c7b-d85b-5b974a37b708"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pywt\n",
        "from skimage import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load noisy image\n",
        "image = cv2.imread('noise.jpg', cv2.IMREAD_COLOR)\n",
        "\n",
        "# 1. Median Filter Denoising\n",
        "median_denoised = cv2.medianBlur(image, 5)\n",
        "\n",
        "# 2. Wavelet Denoising\n",
        "def wavelet_denoising(img):\n",
        "    coeffs = pywt.wavedec2(img.astype(np.float32), 'haar', level=2)\n",
        "    coeffs_thresholded = [coeffs[0]]\n",
        "    for detail_level in coeffs[1:]:\n",
        "        thresholded = tuple(pywt.threshold(c, 0.1, mode='soft') for c in detail_level)\n",
        "        coeffs_thresholded.append(thresholded)\n",
        "\n",
        "    denoised = pywt.waverec2(coeffs_thresholded, 'haar')\n",
        "    denoised = np.clip(denoised, 0, 255).astype(np.uint8)\n",
        "    denoised = cv2.resize(denoised, (img.shape[1], img.shape[0]))\n",
        "    return denoised\n",
        "\n",
        "# 3. Noise2Void placeholder\n",
        "def noise2void_denoising(img):\n",
        "    return img  # Replace with actual denoised output from Noise2Void model\n",
        "\n",
        "# Denoise\n",
        "wavelet_denoised = wavelet_denoising(image)\n",
        "noise2void_denoised = noise2void_denoising(image)\n",
        "\n",
        "# Convert to RGB for visualization and metrics\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "median_rgb = cv2.cvtColor(median_denoised, cv2.COLOR_BGR2RGB)\n",
        "wavelet_rgb = cv2.cvtColor(wavelet_denoised, cv2.COLOR_BGR2RGB)\n",
        "noise2void_rgb = cv2.cvtColor(noise2void_denoised, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Metric Calculation\n",
        "def calculate_metrics(original, denoised):\n",
        "    psnr = metrics.peak_signal_noise_ratio(original, denoised)\n",
        "    ssim = metrics.structural_similarity(original, denoised, channel_axis=-1, win_size=5)\n",
        "    mse = metrics.mean_squared_error(original, denoised)\n",
        "    return psnr, ssim, mse\n",
        "\n",
        "# Metrics\n",
        "psnr_median, ssim_median, mse_median = calculate_metrics(image_rgb, median_rgb)\n",
        "psnr_wavelet, ssim_wavelet, mse_wavelet = calculate_metrics(image_rgb, wavelet_rgb)\n",
        "psnr_noise2void, ssim_noise2void, mse_noise2void = calculate_metrics(image_rgb, noise2void_rgb)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Median Filter     - PSNR: {psnr_median:.2f}, SSIM: {ssim_median:.4f}, MSE: {mse_median:.2f}\")\n",
        "print(f\"Wavelet Denoising - PSNR: {psnr_wavelet:.2f}, SSIM: {ssim_wavelet:.4f}, MSE: {mse_wavelet:.2f}\")\n",
        "print(f\"Noise2Void        - PSNR: {psnr_noise2void:.2f}, SSIM: {ssim_noise2void:.4f}, MSE: {mse_noise2void:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "S9kKS4SyOvTR",
        "outputId": "61abdaec-c94e-4da6-db03-93daf90f51d3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display the images\n",
        "titles = ['Original', 'Median Filter', 'Wavelet Denoising', 'Noise2Void']\n",
        "images = [image_rgb, median_rgb, wavelet_rgb, noise2void_rgb]\n",
        "\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i in range(4):\n",
        "    plt.subplot(1, 4, i+1)\n",
        "    plt.imshow(images[i])\n",
        "    plt.title(titles[i])\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nYitxZyZB4I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "DYWgHWbfacz8",
        "outputId": "285910d8-36c8-4d24-ac7d-24b40893e6dd"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Create a directory to store the extracted frames\n",
        "output_dir = '/content/video_frames'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Upload a video file (for Google Colab)\n",
        "print(\"Please upload a video file (preferably .mp4)\")\n",
        "uploaded = files.upload()\n",
        "video_filename = list(uploaded.keys())[0]\n",
        "print(f\"Video uploaded: {video_filename}\")\n",
        "\n",
        "# Alternative: If you want to use a sample video for testing\n",
        "# Uncomment this section if you don't want to upload a video\n",
        "\"\"\"\n",
        "# Download a sample video if needed\n",
        "!wget -q -O sample_video.mp4 https://github.com/opencv/opencv_extra/raw/master/testdata/highgui/video/big_buck_bunny.mp4\n",
        "video_filename = \"sample_video.mp4\"\n",
        "\"\"\"\n",
        "\n",
        "def extract_frames(video_path, output_folder, frame_interval=1):\n",
        "    \"\"\"\n",
        "    Extract frames from a video and save them as JPEG files\n",
        "\n",
        "    Parameters:\n",
        "    video_path (str): Path to the video file\n",
        "    output_folder (str): Folder to save extracted frames\n",
        "    frame_interval (int): Extract every nth frame\n",
        "    \"\"\"\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if video opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    duration = frame_count / fps\n",
        "\n",
        "    print(f\"Video properties:\")\n",
        "    print(f\"- Total frames: {frame_count}\")\n",
        "    print(f\"- FPS: {fps:.2f}\")\n",
        "    print(f\"- Duration: {duration:.2f} seconds\")\n",
        "\n",
        "    # Initialize frame counter\n",
        "    count = 0\n",
        "    saved_count = 0\n",
        "\n",
        "    # Loop through the video frames\n",
        "    for _ in tqdm(range(frame_count), desc=\"Extracting frames\"):\n",
        "        # Read the next frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If frame is read correctly ret is True\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Save frame if it's at the specified interval\n",
        "        if count % frame_interval == 0:\n",
        "            # Create the output filename\n",
        "            frame_filename = f\"frame_{saved_count:04d}.jpg\"\n",
        "            frame_path = os.path.join(output_folder, frame_filename)\n",
        "\n",
        "            # Save the frame as JPEG file\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            saved_count += 1\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Extracted {saved_count} frames to {output_folder}\")\n",
        "    return saved_count\n",
        "\n",
        "# Extract frames from the uploaded video (every 5th frame)\n",
        "num_frames = extract_frames(video_filename, output_dir, frame_interval=5)\n",
        "\n",
        "# Display some of the extracted frames\n",
        "def display_extracted_frames(frames_dir, num_to_display=5):\n",
        "    \"\"\"Display a sample of extracted frames\"\"\"\n",
        "    # Get all frame files\n",
        "    frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    # Limit the number to display\n",
        "    num_to_display = min(num_to_display, len(frame_files))\n",
        "\n",
        "    if num_to_display == 0:\n",
        "        print(\"No frames to display!\")\n",
        "        return\n",
        "\n",
        "    # Select frames evenly distributed among all extracted frames\n",
        "    indices = np.linspace(0, len(frame_files)-1, num_to_display, dtype=int)\n",
        "    selected_frames = [frame_files[i] for i in indices]\n",
        "\n",
        "    # Display the frames\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, frame_file in enumerate(selected_frames):\n",
        "        img_path = os.path.join(frames_dir, frame_file)\n",
        "        img = cv2.imread(img_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
        "\n",
        "        plt.subplot(1, num_to_display, i+1)\n",
        "        plt.imshow(img_rgb)\n",
        "        plt.title(f\"Frame {frame_file}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display a sample of the extracted frames\n",
        "display_extracted_frames(output_dir, num_to_display=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-ybZtUWpbCfy",
        "outputId": "576861d7-d4fb-46f8-8d4e-8ac2e92a27e7"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create directories for processed frames\n",
        "processed_dir = '/content/processed_frames'\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories for each processing technique\n",
        "adaptive_thresh_dir = os.path.join(processed_dir, 'adaptive_thresh')\n",
        "gaussian_smooth_dir = os.path.join(processed_dir, 'gaussian_smooth')\n",
        "canny_edge_dir = os.path.join(processed_dir, 'canny_edge')\n",
        "bitwise_not_dir = os.path.join(processed_dir, 'bitwise_not')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for dir_path in [adaptive_thresh_dir, gaussian_smooth_dir, canny_edge_dir, bitwise_not_dir]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "# Source directory with extracted frames\n",
        "frames_dir = '/content/video_frames'\n",
        "\n",
        "# Process all frames with different techniques\n",
        "def process_frames(input_dir):\n",
        "    \"\"\"Apply different image processing techniques to all frames\"\"\"\n",
        "    # Get all frame files\n",
        "    frame_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    for frame_file in tqdm(frame_files, desc=\"Processing frames\"):\n",
        "        # Read the frame\n",
        "        img_path = os.path.join(input_dir, frame_file)\n",
        "        frame = cv2.imread(img_path)\n",
        "\n",
        "        if frame is None:\n",
        "            print(f\"Warning: Could not read {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # Convert to grayscale for some operations\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # 1. Adaptive Thresholding\n",
        "        adaptive_thresh = cv2.adaptiveThreshold(\n",
        "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "        # Convert back to BGR for video creation consistency\n",
        "        adaptive_thresh_bgr = cv2.cvtColor(adaptive_thresh, cv2.COLOR_GRAY2BGR)\n",
        "        cv2.imwrite(os.path.join(adaptive_thresh_dir, frame_file), adaptive_thresh_bgr)\n",
        "\n",
        "        # 2. Gaussian Smoothing\n",
        "        gaussian_blur = cv2.GaussianBlur(frame, (15, 15), 0)\n",
        "        cv2.imwrite(os.path.join(gaussian_smooth_dir, frame_file), gaussian_blur)\n",
        "\n",
        "        # 3. Canny Edge Detection\n",
        "        edges = cv2.Canny(gray, 100, 200)\n",
        "        # Convert back to BGR for video creation consistency\n",
        "        edges_bgr = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
        "        cv2.imwrite(os.path.join(canny_edge_dir, frame_file), edges_bgr)\n",
        "\n",
        "        # 4. Bitwise NOT\n",
        "        bitwise_not = cv2.bitwise_not(frame)\n",
        "        cv2.imwrite(os.path.join(bitwise_not_dir, frame_file), bitwise_not)\n",
        "\n",
        "# Process all frames\n",
        "process_frames(frames_dir)\n",
        "\n",
        "# Display sample results from each processing technique\n",
        "def display_processing_results():\n",
        "    \"\"\"Display sample results from each processing technique\"\"\"\n",
        "    # Get a sample frame from the middle of the sequence\n",
        "    frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])\n",
        "    if not frame_files:\n",
        "        print(\"No frames to display!\")\n",
        "        return\n",
        "\n",
        "    # Choose a frame from the middle\n",
        "    sample_frame = frame_files[len(frame_files) // 2]\n",
        "\n",
        "    # Read the original and processed frames\n",
        "    original = cv2.imread(os.path.join(frames_dir, sample_frame))\n",
        "    adaptive = cv2.imread(os.path.join(adaptive_thresh_dir, sample_frame))\n",
        "    gaussian = cv2.imread(os.path.join(gaussian_smooth_dir, sample_frame))\n",
        "    canny = cv2.imread(os.path.join(canny_edge_dir, sample_frame))\n",
        "    bitwise = cv2.imread(os.path.join(bitwise_not_dir, sample_frame))\n",
        "\n",
        "    # Convert BGR to RGB for display\n",
        "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
        "    adaptive = cv2.cvtColor(adaptive, cv2.COLOR_BGR2RGB)\n",
        "    gaussian = cv2.cvtColor(gaussian, cv2.COLOR_BGR2RGB)\n",
        "    canny = cv2.cvtColor(canny, cv2.COLOR_BGR2RGB)\n",
        "    bitwise = cv2.cvtColor(bitwise, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the frames\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    plt.subplot(1, 5, 1)\n",
        "    plt.imshow(original)\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 5, 2)\n",
        "    plt.imshow(adaptive)\n",
        "    plt.title(\"Adaptive Threshold\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 5, 3)\n",
        "    plt.imshow(gaussian)\n",
        "    plt.title(\"Gaussian Blur\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 5, 4)\n",
        "    plt.imshow(canny)\n",
        "    plt.title(\"Canny Edge\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 5, 5)\n",
        "    plt.imshow(bitwise)\n",
        "    plt.title(\"Bitwise NOT\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display processing results\n",
        "display_processing_results()\n",
        "\n",
        "# Create videos from processed frames\n",
        "def create_video_from_frames(frames_dir, output_video_path, fps=30):\n",
        "    \"\"\"Create a video from frames in a directory\"\"\"\n",
        "    # Get all frame files\n",
        "    frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    if not frame_files:\n",
        "        print(f\"No frames found in {frames_dir}\")\n",
        "        return\n",
        "\n",
        "    # Read the first frame to get dimensions\n",
        "    first_frame = cv2.imread(os.path.join(frames_dir, frame_files[0]))\n",
        "    height, width, layers = first_frame.shape\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # mp4 format\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Add frames to the video\n",
        "    for frame_file in tqdm(frame_files, desc=f\"Creating {os.path.basename(output_video_path)}\"):\n",
        "        frame_path = os.path.join(frames_dir, frame_file)\n",
        "        frame = cv2.imread(frame_path)\n",
        "        if frame is not None:\n",
        "            out.write(frame)\n",
        "\n",
        "    # Release the video writer\n",
        "    out.release()\n",
        "    print(f\"Video saved: {output_video_path}\")\n",
        "\n",
        "# Create output directory for videos\n",
        "videos_dir = '/content/processed_videos'\n",
        "os.makedirs(videos_dir, exist_ok=True)\n",
        "\n",
        "# Create videos from each set of processed frames\n",
        "processing_techniques = {\n",
        "    'original': frames_dir,\n",
        "    'adaptive_threshold': adaptive_thresh_dir,\n",
        "    'gaussian_blur': gaussian_smooth_dir,\n",
        "    'canny_edge': canny_edge_dir,\n",
        "    'bitwise_not': bitwise_not_dir\n",
        "}\n",
        "\n",
        "for name, dir_path in processing_techniques.items():\n",
        "    output_video = os.path.join(videos_dir, f\"{name}_video.mp4\")\n",
        "    create_video_from_frames(dir_path, output_video, fps=24)\n",
        "\n",
        "# Create a collage from video frames\n",
        "def create_frame_collage(frames_dir, output_path, grid_size=(3, 4)):\n",
        "    \"\"\"Create a collage from video frames\"\"\"\n",
        "    # Get all frame files\n",
        "    frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    if not frame_files:\n",
        "        print(f\"No frames found in {frames_dir}\")\n",
        "        return\n",
        "\n",
        "    # Determine how many frames to use\n",
        "    rows, cols = grid_size\n",
        "    num_frames_needed = rows * cols\n",
        "\n",
        "    # Select frames evenly throughout the sequence\n",
        "    if len(frame_files) <= num_frames_needed:\n",
        "        selected_frames = frame_files\n",
        "    else:\n",
        "        indices = np.linspace(0, len(frame_files) - 1, num_frames_needed, dtype=int)\n",
        "        selected_frames = [frame_files[i] for i in indices]\n",
        "\n",
        "    # Read the first frame to get dimensions\n",
        "    first_frame = cv2.imread(os.path.join(frames_dir, frame_files[0]))\n",
        "    frame_height, frame_width, _ = first_frame.shape\n",
        "\n",
        "    # Create a blank canvas\n",
        "    collage_width = frame_width * cols\n",
        "    collage_height = frame_height * rows\n",
        "    collage = np.zeros((collage_height, collage_width, 3), dtype=np.uint8)\n",
        "\n",
        "    # Place frames in the collage\n",
        "    for idx, frame_file in enumerate(selected_frames):\n",
        "        if idx >= num_frames_needed:\n",
        "            break\n",
        "\n",
        "        # Calculate position in grid\n",
        "        row = idx // cols\n",
        "        col = idx % cols\n",
        "\n",
        "        # Read the frame\n",
        "        frame = cv2.imread(os.path.join(frames_dir, frame_file))\n",
        "\n",
        "        if frame is None:\n",
        "            continue\n",
        "\n",
        "        # Calculate position in collage\n",
        "        y_start = row * frame_height\n",
        "        y_end = (row + 1) * frame_height\n",
        "        x_start = col * frame_width\n",
        "        x_end = (col + 1) * frame_width\n",
        "\n",
        "        # Place the frame in the collage\n",
        "        collage[y_start:y_end, x_start:x_end] = frame\n",
        "\n",
        "    # Save the collage\n",
        "    cv2.imwrite(output_path, collage)\n",
        "    print(f\"Collage saved: {output_path}\")\n",
        "\n",
        "    # Return the collage for display\n",
        "    return cv2.cvtColor(collage, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Create directory for collages\n",
        "collages_dir = '/content/collages'\n",
        "os.makedirs(collages_dir, exist_ok=True)\n",
        "\n",
        "# Create collages for original and processed frames\n",
        "collages = {}\n",
        "for name, dir_path in processing_techniques.items():\n",
        "    output_path = os.path.join(collages_dir, f\"{name}_collage.jpg\")\n",
        "    collages[name] = create_frame_collage(dir_path, output_path)\n",
        "\n",
        "# Display the collages\n",
        "def display_collages():\n",
        "    \"\"\"Display all created collages\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Calculate grid size based on number of collages\n",
        "    num_collages = len(collages)\n",
        "    rows = (num_collages + 1) // 2  # Round up division\n",
        "\n",
        "    for i, (name, collage) in enumerate(collages.items()):\n",
        "        plt.subplot(rows, 2, i + 1)\n",
        "        plt.imshow(collage)\n",
        "        plt.title(f\"{name.replace('_', ' ').title()} Collage\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display all collages\n",
        "display_collages()\n",
        "\n",
        "# Print summary of created files\n",
        "print(\"\\n----- Summary of Created Files -----\")\n",
        "print(f\"1. Extracted frames: {len(os.listdir(frames_dir))} frames in {frames_dir}\")\n",
        "\n",
        "print(\"\\n2. Processed frames:\")\n",
        "for name, dir_path in processing_techniques.items():\n",
        "    if name != 'original':\n",
        "        print(f\"   - {name.replace('_', ' ').title()}: {len(os.listdir(dir_path))} frames in {dir_path}\")\n",
        "\n",
        "print(\"\\n3. Created videos:\")\n",
        "for name in processing_techniques.keys():\n",
        "    print(f\"   - {name.replace('_', ' ').title()}: {videos_dir}/{name}_video.mp4\")\n",
        "\n",
        "print(\"\\n4. Created collages:\")\n",
        "for name in processing_techniques.keys():\n",
        "    print(f\"   - {name.replace('_', ' ').title()}: {collages_dir}/{name}_collage.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "tyo-_pQ7SIk2",
        "outputId": "f91cf4d9-6f01-4c87-9a74-4d6e8d5e715e"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json\n",
        "\n",
        "# Setup Kaggle API\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYgl-cMncoWw",
        "outputId": "3b6ead24-4c37-45eb-8382-a97bf018909d"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"pevogam/ucf101\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yliH5IJWcrmU",
        "outputId": "5dec91e8-157b-4be9-9c82-fa008c06f5fc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check the folder structure under UCF101\n",
        "dataset_path = '/kaggle/input/ucf101/UCF101'\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    print(\"üìÅ\", root)\n",
        "    for d in dirs:\n",
        "        print(\" ‚î£‚îÅ\", d)\n",
        "    for f in files[:5]:  # Show only first 5 files to avoid overload\n",
        "        print(\" ‚îó‚îÅ\", f)\n",
        "    break  # Only show top-level content for now\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gJ4QgDyct5w",
        "outputId": "3b31838f-5c03-4945-fbb2-5a853bb2b2b9"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os, shutil, random\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"pevogam/ucf101\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# ‚úÖ Corrected source path based on your folder structure\n",
        "SOURCE_DIR = os.path.join(path, 'UCF101', 'UCF-101')  # Corrected path\n",
        "DEST_DIR = './UCF101_subset'\n",
        "SELECTED_CLASSES = ['Basketball', 'Biking', 'PlayingGuitar', 'Typing', 'JumpRope']\n",
        "VIDEOS_PER_CLASS = 10\n",
        "\n",
        "os.makedirs(DEST_DIR, exist_ok=True)\n",
        "\n",
        "for cls in SELECTED_CLASSES:\n",
        "    class_path = os.path.join(SOURCE_DIR, cls)\n",
        "    dest_class_path = os.path.join(DEST_DIR, cls)\n",
        "    os.makedirs(dest_class_path, exist_ok=True)\n",
        "\n",
        "    selected = random.sample(os.listdir(class_path), VIDEOS_PER_CLASS)\n",
        "    for video in selected:\n",
        "        shutil.copy(os.path.join(class_path, video), dest_class_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBfrSHP-cwiP",
        "outputId": "a75433fe-e441-44e4-ad30-043ebb0d9158"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Define paths and parameters\n",
        "DATA_DIR = './UCF101_subset'\n",
        "FRAME_SIZE = (112, 112)  # Width, Height\n",
        "SEQUENCE_LENGTH = 16  # Number of frames to use per video\n",
        "FRAME_SKIP = 5  # Extract every 5th frame\n",
        "\n",
        "def extract_frames(video_path, frame_size, sequence_length, frame_skip):\n",
        "    \"\"\"Extract frames from a video and return as a numpy array.\"\"\"\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frame_count = 0\n",
        "    success = True\n",
        "\n",
        "    while success:\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_skip == 0:\n",
        "            # Convert from BGR to RGB\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            # Resize frame\n",
        "            frame = cv2.resize(frame, frame_size)\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == sequence_length:\n",
        "                break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # If we don't have enough frames, pad with zeros\n",
        "    if len(frames) < sequence_length:\n",
        "        frames.extend([np.zeros((frame_size[1], frame_size[0], 3), dtype=np.uint8)] * (sequence_length - len(frames)))\n",
        "\n",
        "    # Convert to numpy array and normalize\n",
        "    frames = np.array(frames).astype(np.float32) / 255.0\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "X = []  # Will contain frame sequences\n",
        "y = []  # Will contain class labels\n",
        "class_names = []\n",
        "\n",
        "import os\n",
        "for class_name in os.listdir(DATA_DIR):\n",
        "    class_dir = os.path.join(DATA_DIR, class_name)\n",
        "    class_names.append(class_name)\n",
        "\n",
        "    for video_file in os.listdir(class_dir):\n",
        "        video_path = os.path.join(class_dir, video_file)\n",
        "        print(f\"Processing {video_path}\")\n",
        "\n",
        "        # Extract frames from the video\n",
        "        frames = extract_frames(video_path, FRAME_SIZE, SEQUENCE_LENGTH, FRAME_SKIP)\n",
        "\n",
        "        X.append(frames)\n",
        "        y.append(class_name)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# Encode class labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "print(f\"Classes: {label_encoder.classes_}\")\n",
        "\n",
        "# Split into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_categorical, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Testing set: {X_test.shape}, {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1cvsm2kTdLa3",
        "outputId": "b9d811c7-c005-4843-a1fc-3a19391c443f"
      },
      "outputs": [],
      "source": [
        "# CNN+LSTM Model\n",
        "def build_cnn_lstm_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        # TimeDistributed wrapper applies the same Conv2D layer to each frame\n",
        "        TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=input_shape),\n",
        "        TimeDistributed(MaxPooling2D((2, 2))),\n",
        "        TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n",
        "        TimeDistributed(MaxPooling2D((2, 2))),\n",
        "        TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same')),\n",
        "        TimeDistributed(MaxPooling2D((2, 2))),\n",
        "        TimeDistributed(Flatten()),\n",
        "        LSTM(256, return_sequences=False),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and train the model\n",
        "input_shape = (SEQUENCE_LENGTH, FRAME_SIZE[1], FRAME_SIZE[0], 3)  # (frames, height, width, channels)\n",
        "num_classes = len(class_names)\n",
        "\n",
        "model = build_cnn_lstm_model(input_shape, num_classes)\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=15,\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "report = classification_report(y_true, y_pred,\n",
        "                              target_names=label_encoder.classes_,\n",
        "                              output_dict=True)\n",
        "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Calculate precision, recall and F1-score for each class\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "\n",
        "# Create a dataframe to display metrics by class\n",
        "import pandas as pd\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Class': label_encoder.classes_,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1\n",
        "})\n",
        "print(\"\\nPer-class Metrics:\")\n",
        "print(metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvqMNdwZdRpj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
