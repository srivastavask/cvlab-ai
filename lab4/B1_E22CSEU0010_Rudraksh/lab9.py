# -*- coding: utf-8 -*-
"""lab9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11YqyE85aemnPoDZtH4ksyM1C8st0u_uH

Rudraksh Singh E22CSEU0010 LAB 9

Task-1:-  Image Stitching, Panorama stitching, Structure from motion, Image Inpainting
"""

uploaded = files.upload()

for fn in uploaded.keys():
    img = cv2.imread(fn)

def add_salt_pepper_noise(image, amount=0.04):
    out = image.copy()
    num_salt = np.ceil(amount * image.size * 0.5).astype(int)
    coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape]
    out[coords[0], coords[1], :] = 255
    num_pepper = np.ceil(amount * image.size * 0.5).astype(int)
    coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape]
    out[coords[0], coords[1], :] = 0
    return out

noisy = add_salt_pepper_noise(img)
cv2_imshow(noisy)

gray = cv2.cvtColor(noisy, cv2.COLOR_BGR2GRAY)
_, mask = cv2.threshold(gray, 250, 255, cv2.THRESH_BINARY)

dst_telea = cv2.inpaint(noisy, mask, 3, cv2.INPAINT_TELEA)
dst_ns = cv2.inpaint(noisy, mask, 3, cv2.INPAINT_NS)

print("Inpainted with Telea:")
cv2_imshow(dst_telea)

print("Inpainted with Navier-Stokes:")
cv2_imshow(dst_ns)

uploaded = files.upload()
imgs = [cv2.imread(fn) for fn in uploaded.keys()]
print(f"{len(imgs)} images loaded.")

stitcher = cv2.Stitcher_create()
status, stitched = stitcher.stitch(imgs)

if status == cv2.Stitcher_OK:
    print("Stitching successful!")
    cv2_imshow(stitched)
else:
    print(f"Stitching failed with status code: {status}")

uploaded = files.upload()
img_names = list(uploaded.keys())
img1 = cv2.imread(img_names[0])
img2 = cv2.imread(img_names[1])
orb = cv2.ORB_create()
k1, d1 = orb.detectAndCompute(img1, None)
k2, d2 = orb.detectAndCompute(img2, None)

bf = cv2.BFMatcher()
matches = bf.knnMatch(d1, d2, k=2)
good = [m for m,n in matches if m.distance < 0.75 * n.distance]

src_pts = np.float32([k1[m.queryIdx].pt for m in good]).reshape(-1,1,2)
dst_pts = np.float32([k2[m.trainIdx].pt for m in good]).reshape(-1,1,2)

H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
width = img1.shape[1] + img2.shape[1]
height = max(img1.shape[0], img2.shape[0])
panorama = cv2.warpPerspective(img1, H, (width, height))
panorama[0:img2.shape[0], 0:img2.shape[1]] = img2

print("Manual Panorama Result:")
cv2_imshow(panorama)

from google.colab import files
uploaded = files.upload()

img_names = list(uploaded.keys())
img1_color = cv2.imread(img_names[0])
img2_color = cv2.imread(img_names[1])

img1 = cv2.cvtColor(img1_color, cv2.COLOR_BGR2GRAY)
img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2GRAY)

sift = cv2.SIFT_create()
kp1, des1 = sift.detectAndCompute(img1, None)
kp2, des2 = sift.detectAndCompute(img2, None)

bf = cv2.BFMatcher()
matches = bf.knnMatch(des1, des2, k=2)
good = [m for m,n in matches if m.distance < 0.75 * n.distance]


img_matches = cv2.drawMatches(
    img1_color, kp1,
    img2_color, kp2,
    good[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
)
cv2_imshow(img_matches)

pts1 = np.float32([kp1[m.queryIdx].pt for m in good])
pts2 = np.float32([kp2[m.trainIdx].pt for m in good])

F, mask_f = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC)
print("Fundamental Matrix:\n", F)

# Sample intrinsics
K = np.array([[800, 0, 320], [0, 800, 240], [0, 0, 1]])

E, mask_e = cv2.findEssentialMat(pts1, pts2, K)
print("Essential Matrix:\n", E)

_, R, t, mask_pose = cv2.recoverPose(E, pts1, pts2, K)
print("Recovered Rotation:\n", R)
print("Recovered Translation:\n", t)

"""Task-2:-  Image Restoration using Autoencoder Model

"""

# noisy versions using TF operations
noise_factor = 0.2
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)

# TF clip operations [0, 1]
x_train_noisy = tf.clip_by_value(x_train_noisy, 0., 1.)
x_test_noisy = tf.clip_by_value(x_test_noisy, 0., 1.)

# examples
n = 5
plt.figure(figsize=(20, 4))
for i in range(n):
    # Original image
    plt.subplot(2, n, i+1)
    plt.imshow(tf.squeeze(x_test[i]), cmap='gray')
    plt.title('Original')
    plt.axis('off')

    # Noisy image
    plt.subplot(2, n, i+n+1)
    plt.imshow(tf.squeeze(x_test_noisy[i]), cmap='gray')
    plt.title('Noisy')
    plt.axis('off')
plt.tight_layout()
plt.show()

# autoencoder using Keras Functional API
input_img = Input(shape=(28, 28, 1))

x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
x = MaxPooling2D((2, 2), padding='same')(x)

x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)
decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)
autoencoder = Model(input_img, decoded)
autoencoder.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mean_squared_error'
)
autoencoder.summary()

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)
history = autoencoder.fit(
    x_train_noisy, x_train,
    epochs=20,
    batch_size=128,
    shuffle=True,
    validation_data=(x_test_noisy, x_test),
    callbacks=[early_stopping],
    verbose=1
)

tf.experimental.numpy.experimental_enable_numpy_behavior()
decoded_imgs = autoencoder.predict(x_test_noisy)

# shape and values of decoded images
print(f"Decoded images shape: {decoded_imgs.shape}")
print(f"Min value in decoded: {decoded_imgs.min()}, Max value: {decoded_imgs.max()}")

# PSNR
psnr_values = []
for i in range(n):
    psnr_value = peak_signal_noise_ratio(x_test[i], decoded_imgs[i])
    psnr_values.append(psnr_value)

print(f"PSNR values for first {n} images: {psnr_values}")
print(f"Average PSNR: {np.mean(psnr_values):.2f} dB")

# results
plt.figure(figsize=(20, 6))
for i in range(n):
    # Original
    plt.subplot(3, n, i+1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    plt.title('Original')
    plt.axis('off')

    # Noisy
    plt.subplot(3, n, i+n+1)
    plt.imshow(x_test_noisy[i].reshape(28, 28), cmap='gray')
    plt.title('Noisy')
    plt.axis('off')

    # Denoised
    plt.subplot(3, n, i+2*n+1)
    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')
    plt.title(f'Denoised (PSNR: {psnr_values[i]:.2f} dB)')
    plt.axis('off')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 4))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss During Training')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.grid(True)
plt.show()

sample_results = np.hstack([
    x_test[:5].reshape(-1, 28),
    x_test_noisy[:5].reshape(-1, 28),
    decoded_imgs[:5].reshape(-1, 28)
])
plt.figure(figsize=(8, 8))
plt.imshow(sample_results, cmap='gray')
plt.title('Original | Noisy | Denoised')
plt.axis('off')
plt.tight_layout()
plt.savefig('denoising_results.png')
plt.show()

"""Generative adversarial networks on MNIST and Cifar-10 datasets"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist, cifar10

# Set random seed
tf.random.set_seed(42)
np.random.seed(42)

# Parameters
batch_size = 128
latent_dim = 100
epochs = 50

# Load and preprocess MNIST data
(mnist_images, _), (_, _) = mnist.load_data()
mnist_images = mnist_images.reshape(mnist_images.shape[0], 28, 28, 1).astype('float32')
mnist_images = (mnist_images - 127.5) / 127.5  # Normalize to [-1, 1]
mnist_dataset = tf.data.Dataset.from_tensor_slices(mnist_images).shuffle(60000).batch(batch_size)

# Load and preprocess CIFAR-10 data
(cifar_images, _), (_, _) = cifar10.load_data()
cifar_images = cifar_images.astype('float32')
cifar_images = (cifar_images - 127.5) / 127.5  # Normalize to [-1, 1]
cifar_dataset = tf.data.Dataset.from_tensor_slices(cifar_images).shuffle(50000).batch(batch_size)

# MNIST Generator
mnist_generator = models.Sequential([
    layers.Dense(7 * 7 * 256, input_shape=(latent_dim,), use_bias=False),
    layers.BatchNormalization(),
    layers.LeakyReLU(0.2),
    layers.Reshape((7, 7, 256)),

    layers.Conv2DTranspose(128, 5, strides=1, padding='same', use_bias=False),
    layers.BatchNormalization(),
    layers.LeakyReLU(0.2),

    layers.Conv2DTranspose(64, 5, strides=2, padding='same', use_bias=False),
    layers.BatchNormalization(),
    layers.LeakyReLU(0.2),

    layers.Conv2DTranspose(1, 5, strides=2, padding='same', use_bias=False, activation='tanh')
])

# MNIST Discriminator
mnist_discriminator = models.Sequential([
    layers.Conv2D(64, 5, strides=2, padding='same', input_shape=[28, 28, 1]),
    layers.LeakyReLU(0.2),
    layers.Dropout(0.3),

    layers.Conv2D(128, 5, strides=2, padding='same'),
    layers.LeakyReLU(0.2),
    layers.Dropout(0.3),

    layers.Flatten(),
    layers.Dense(1)
])

# CIFAR Generator
cifar_generator = models.Sequential([
    layers.Dense(4 * 4 * 512, input_shape=(latent_dim,), use_bias=False),
    layers.BatchNormalization(),
    layers.LeakyReLU(0.2),
    layers.Reshape((4, 4, 512)),

    layers.Conv2DTranspose(256, 5, strides=2, padding='same', use_bias=False),
    layers.BatchNormalization(),
    layers.LeakyReLU(0.2),

    layers.Conv2DTranspose(128, 5, strides=2, padding='same', use_bias=False),
    layers.BatchNormalization(),
    layers.LeakyReLU(0.2),

    layers.Conv2DTranspose(64, 5, strides=2, padding='same', use_bias=False),
    layers.BatchNormalization(),
    layers.LeakyReLU(0.2),

    layers.Conv2DTranspose(3, 5, strides=1, padding='same', use_bias=False, activation='tanh')
])

# CIFAR Discriminator
cifar_discriminator = models.Sequential([
    layers.Conv2D(64, 5, strides=2, padding='same', input_shape=[32, 32, 3]),
    layers.LeakyReLU(0.2),
    layers.Dropout(0.3),

    layers.Conv2D(128, 5, strides=2, padding='same'),
    layers.LeakyReLU(0.2),
    layers.Dropout(0.3),

    layers.Conv2D(256, 5, strides=2, padding='same'),
    layers.LeakyReLU(0.2),
    layers.Dropout(0.3),

    layers.Flatten(),
    layers.Dense(1)
])

# Loss function
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

# Optimizers
mnist_gen_optimizer = tf.keras.optimizers.Adam(1e-4)
mnist_disc_optimizer = tf.keras.optimizers.Adam(1e-4)
cifar_gen_optimizer = tf.keras.optimizers.Adam(1e-4)
cifar_disc_optimizer = tf.keras.optimizers.Adam(1e-4)

# Seed for visualization
mnist_seed = tf.random.normal([10, latent_dim])
cifar_seed = tf.random.normal([10, latent_dim])
# Training MNIST GAN
@tf.function
def train_mnist_step(images):
    noise = tf.random.normal([batch_size, latent_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = mnist_generator(noise, training=True)

        real_output = mnist_discriminator(images, training=True)
        fake_output = mnist_discriminator(generated_images, training=True)

        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)
        disc_loss = cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(fake_output), fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, mnist_generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, mnist_discriminator.trainable_variables)

    mnist_gen_optimizer.apply_gradients(zip(gradients_of_generator, mnist_generator.trainable_variables))
    mnist_disc_optimizer.apply_gradients(zip(gradients_of_discriminator, mnist_discriminator.trainable_variables))

    return gen_loss, disc_loss

# Training CIFAR GAN
@tf.function
def train_cifar_step(images):
    noise = tf.random.normal([batch_size, latent_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = cifar_generator(noise, training=True)

        real_output = cifar_discriminator(images, training=True)
        fake_output = cifar_discriminator(generated_images, training=True)

        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)
        disc_loss = cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(fake_output), fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, cifar_generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, cifar_discriminator.trainable_variables)

    cifar_gen_optimizer.apply_gradients(zip(gradients_of_generator, cifar_generator.trainable_variables))
    cifar_disc_optimizer.apply_gradients(zip(gradients_of_discriminator, cifar_discriminator.trainable_variables))

    return gen_loss, disc_loss

# Training loop
mnist_gen_losses = []
mnist_disc_losses = []
cifar_gen_losses = []
cifar_disc_losses = []

print("Training GAN on MNIST dataset...")
for epoch in range(epochs):
    mnist_g_loss_epoch = 0
    mnist_d_loss_epoch = 0
    mnist_steps = 0

    for image_batch in mnist_dataset:
        g_loss, d_loss = train_mnist_step(image_batch)
        mnist_g_loss_epoch += g_loss
        mnist_d_loss_epoch += d_loss
        mnist_steps += 1

    mnist_gen_losses.append(mnist_g_loss_epoch/mnist_steps)
    mnist_disc_losses.append(mnist_d_loss_epoch/mnist_steps)

    if (epoch + 1) % 10 == 0:
        print(f"MNIST Epoch {epoch+1}/{epochs}, Gen Loss: {mnist_gen_losses[-1]}, Disc Loss: {mnist_disc_losses[-1]}")

print("\nTraining GAN on CIFAR-10 dataset...")
for epoch in range(epochs):
    cifar_g_loss_epoch = 0
    cifar_d_loss_epoch = 0
    cifar_steps = 0

    for image_batch in cifar_dataset:
        g_loss, d_loss = train_cifar_step(image_batch)
        cifar_g_loss_epoch += g_loss
        cifar_d_loss_epoch += d_loss
        cifar_steps += 1

    cifar_gen_losses.append(cifar_g_loss_epoch/cifar_steps)
    cifar_disc_losses.append(cifar_d_loss_epoch/cifar_steps)

    if (epoch + 1) % 10 == 0:
        print(f"CIFAR Epoch {epoch+1}/{epochs}, Gen Loss: {cifar_gen_losses[-1]}, Disc Loss: {cifar_disc_losses[-1]}")

# Generate final images
mnist_generated_images = mnist_generator(mnist_seed, training=False)
cifar_generated_images = cifar_generator(cifar_seed, training=False)

# Plot the losses
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(mnist_gen_losses, label='Generator')
plt.plot(mnist_disc_losses, label='Discriminator')
plt.title('MNIST GAN Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(cifar_gen_losses, label='Generator')
plt.plot(cifar_disc_losses, label='Discriminator')
plt.title('CIFAR-10 GAN Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig('gan_losses.png')
plt.show()

# Display generated images
plt.figure(figsize=(20, 4))

# MNIST images
plt.subplot(1, 2, 1)
for i in range(10):
    plt.subplot(2, 10, i+1)
    plt.imshow(mnist_generated_images[i, :, :, 0] * 0.5 + 0.5, cmap='gray')
    plt.axis('off')

# CIFAR images
for i in range(10):
    plt.subplot(2, 10, i+11)
    plt.imshow((cifar_generated_images[i] * 0.5 + 0.5).numpy())
    plt.axis('off')

plt.tight_layout()
plt.savefig('generated_images.png')
plt.show()

"""Unet for image segmentation"""

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import tensorflow_datasets as tfds

tf.random.set_seed(42)

# Oxford-IIIT Pet dataset
dataset, info = tfds.load('oxford_iiit_pet', with_info=True)

# Parameters
IMG_SIZE = 128
BATCH_SIZE = 32
EPOCHS = 50
BUFFER_SIZE = 1000

# Preprocess images
def preprocess(data):
    input_image = tf.cast(data['image'], tf.float32)
    input_mask = tf.cast(data['segmentation_mask'], tf.float32)

    # Resize
    input_image = tf.image.resize(input_image, [IMG_SIZE, IMG_SIZE])
    input_mask = tf.image.resize(input_mask, [IMG_SIZE, IMG_SIZE])

    # Normalize
    input_image = input_image / 255.0

    #  mask to one-hot
    input_mask = tf.cast(input_mask > 1, tf.float32)

    return input_image, input_mask

#  datasets
train_dataset = dataset['train'].map(preprocess).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_dataset = dataset['test'].map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

def unet_model(output_channels=1):
    # Encoder
    inputs = layers.Input(shape=[IMG_SIZE, IMG_SIZE, 3])

    # Encoder path
    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)
    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)
    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)
    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)
    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)
    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)
    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)
    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)
    drop4 = layers.Dropout(0.5)(conv4)
    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(drop4)

    # Bridge
    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)
    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)
    drop5 = layers.Dropout(0.5)(conv5)

    # Decoder path
    up6 = layers.Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(drop5)
    concat6 = layers.Concatenate()([up6, drop4])
    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(concat6)
    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)

    up7 = layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)
    concat7 = layers.Concatenate()([up7, conv3])
    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(concat7)
    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)

    up8 = layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)
    concat8 = layers.Concatenate()([up8, conv2])
    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(concat8)
    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)

    up9 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)
    concat9 = layers.Concatenate()([up9, conv1])
    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(concat9)
    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)

    # Output layer
    outputs = layers.Conv2D(output_channels, 1, activation='sigmoid')(conv9)

    model = models.Model(inputs=inputs, outputs=outputs)
    return model

# metrics
def dice_coef(y_true, y_pred, smooth=1):
    y_true_f = tf.reshape(y_true, [-1])
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)

def dice_loss(y_true, y_pred):
    return 1 - dice_coef(y_true, y_pred)

def iou(y_true, y_pred, smooth=1):
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection
    return (intersection + smooth) / (union + smooth)

#compile model
model = unet_model()
model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss=dice_loss,
    metrics=[dice_coef, iou, 'binary_accuracy']
)

#callbacks
callbacks = [
    EarlyStopping(patience=10, restore_best_weights=True),
    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=1e-6)
]

# Train
history = model.fit(
    train_dataset,
    epochs=EPOCHS,
    validation_data=test_dataset,
    callbacks=callbacks
)

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(history.history['dice_coef'], label='Train Dice')
plt.plot(history.history['val_dice_coef'], label='Val Dice')
plt.title('Dice Coefficient')
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(history.history['iou'], label='Train IoU')
plt.plot(history.history['val_iou'], label='Val IoU')
plt.title('IoU')
plt.legend()

plt.tight_layout()
plt.savefig('unet_training_history.png')
plt.show()

def display_sample(display_list):
    plt.figure(figsize=(15, 5))
    title = ['Input Image', 'True Mask', 'Predicted Mask']

    for i in range(len(display_list)):
        plt.subplot(1, 3, i+1)
        plt.title(title[i])
        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))
        plt.axis('off')

    plt.tight_layout()
    plt.savefig('unet_prediction_examples.png')
    plt.show()

# predictions
for images, masks in test_dataset.take(3):
    # random image
    idx = np.random.randint(0, images.shape[0])
    sample_image = images[idx]
    sample_mask = masks[idx]

    # Making prediction
    pred_mask = model.predict(sample_image[tf.newaxis, ...])[0]

    # results
    display_sample([sample_image, sample_mask, pred_mask])

# Evaluate model on test set
test_results = model.evaluate(test_dataset)
print("\nTest Results:")
print(f"Loss: {test_results[0]:.4f}")
print(f"Dice Coefficient: {test_results[1]:.4f}")
print(f"IoU: {test_results[2]:.4f}")
print(f"Accuracy: {test_results[3]:.4f}")

"""Mobilenet based image classification"""

from google.colab import files
files.upload()
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d gauravduttakiit/dogs-breed-dataset

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, applications
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import pandas as pd

tf.random.set_seed(42)
DATASET_PATH = './dogs_dataset'
TRAIN_PATH = os.path.join(DATASET_PATH, 'train')
TEST_PATH = os.path.join(DATASET_PATH, 'test')

IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 15
NUM_CLASSES = len(os.listdir(TRAIN_PATH))

# Data generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    TRAIN_PATH,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

validation_generator = train_datagen.flow_from_directory(
    TRAIN_PATH,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

test_generator = test_datagen.flow_from_directory(
    TEST_PATH,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

# Class labels
class_names = list(train_generator.class_indices.keys())
print(f"Dog breeds: {class_names}")

# Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
callbacks = [early_stop, reduce_lr]

results = {}

for version in ['v1', 'v2', 'v3']:
    print(f"\nðŸ“¦ Training MobileNet {version.upper()}...")

    #  model selection
    if version == 'v1':
        base_model = applications.MobileNet(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
    elif version == 'v2':
        base_model = applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
    else:
        base_model = applications.MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))

    base_model.trainable = False

    #  model
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(NUM_CLASSES, activation='softmax')
    ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Train
    history = model.fit(
        train_generator,
        steps_per_epoch=len(train_generator),
        validation_data=validation_generator,
        validation_steps=len(validation_generator),
        epochs=EPOCHS,
        callbacks=callbacks
    )

    # Evaluate
    test_loss, test_acc = model.evaluate(test_generator)
    print(f"âœ… Test accuracy: {test_acc:.4f}")

    results[version] = {
        'history': history.history,
        'test_accuracy': test_acc
    }

# Plot training history
plt.figure(figsize=(12, 10))

# Accuracy
plt.subplot(2, 1, 1)
for version in results:
    plt.plot(results[version]['history']['accuracy'], label=f'MobileNet {version.upper()} Train')
    plt.plot(results[version]['history']['val_accuracy'], label=f'MobileNet {version.upper()} Val')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(2, 1, 2)
for version in results:
    plt.plot(results[version]['history']['loss'], label=f'MobileNet {version.upper()} Train')
    plt.plot(results[version]['history']['val_loss'], label=f'MobileNet {version.upper()} Val')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

comparison = pd.DataFrame({
    'Model Version': [f'MobileNet {v.upper()}' for v in results],
    'Test Accuracy': [results[v]['test_accuracy'] for v in results],
    'Final Train Accuracy': [results[v]['history']['accuracy'][-1] for v in results],
    'Final Val Accuracy': [results[v]['history']['val_accuracy'][-1] for v in results]
})

print("\nðŸ“Š Final Model Comparison:")
print(comparison)